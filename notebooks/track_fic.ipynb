{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸš— Vehicle Speed Detection using YOLOv8\n",
        "\n",
        "This notebook detects vehicles in a video, tracks them across frames,\n",
        "and estimates their speed using perspective transformation and pixel displacement.\n",
        "\n",
        "### Key Features\n",
        "- YOLOv8 object detection\n",
        "- ByteTrack multi-object tracking\n",
        "- Perspective transform for real-world mapping\n",
        "- Speed estimation in km/h\n",
        "- Output annotated video\n",
        "\n",
        "This notebook is designed to be **modular, readable, and production-ready**.\n"
      ],
      "metadata": {
        "id": "WbCBKAUHTF39"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âš¡ Pro Tip: Enable GPU Acceleration\n",
        "\n",
        "If you are running this notebook in **Google Colab**:\n",
        "\n",
        "1. Go to **Edit â†’ Notebook settings**\n",
        "2. Set **Hardware accelerator** to **GPU**\n",
        "3. Click **Save**\n",
        "\n",
        "GPU acceleration significantly improves inference speed.\n"
      ],
      "metadata": {
        "id": "8wMSQNfITP37"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUNzb8x4SThu",
        "outputId": "38c9e505-fd8c-4f14-8d4e-4294ae1a415b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“¦ Install Required Libraries\n",
        "\n",
        "We install all required dependencies:\n",
        "- `ultralytics` for YOLOv8\n",
        "- `supervision` for tracking and annotation\n",
        "- `opencv-python` for video processing\n",
        "- `tqdm` for progress visualization\n"
      ],
      "metadata": {
        "id": "nRQNPtVeTZJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U ultralytics supervision opencv-python tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rb07sAATdjK",
        "outputId": "7e744663-d4bb-4ca1-bc34-68a15c410101"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.4.7-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting supervision\n",
            "  Downloading supervision-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Collecting opencv-python\n",
            "  Downloading opencv_python-4.13.0.90-cp37-abi3-manylinux_2_28_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch<2.10,>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cpu)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cpu)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
            "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from supervision) (0.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2026.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<2.10,>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<2.10,>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<2.10,>=1.8.0->ultralytics) (3.0.3)\n",
            "Downloading ultralytics-8.4.7-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading supervision-0.27.0-py3-none-any.whl (212 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m212.4/212.4 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python-4.13.0.90-cp37-abi3-manylinux_2_28_x86_64.whl (72.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: opencv-python, ultralytics-thop, supervision, ultralytics\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.12.0.88\n",
            "    Uninstalling opencv-python-4.12.0.88:\n",
            "      Successfully uninstalled opencv-python-4.12.0.88\n",
            "Successfully installed opencv-python-4.13.0.90 supervision-0.27.0 ultralytics-8.4.7 ultralytics-thop-2.0.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“š Import Libraries\n",
        "\n",
        "This section imports all Python libraries used for:\n",
        "- Deep learning\n",
        "- Video processing\n",
        "- Tracking\n",
        "- Numerical computation\n"
      ],
      "metadata": {
        "id": "nqdNcX2ETfQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import supervision as sv\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict, deque\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZiwYGk8Thbn",
        "outputId": "08fca05b-c89f-4b55-ca47-c6086e6b7429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âš™ï¸ Configuration Parameters\n",
        "\n",
        "These parameters control:\n",
        "- Model selection\n",
        "- Detection thresholds\n",
        "- Input/output paths\n",
        "- Speed estimation constraints\n"
      ],
      "metadata": {
        "id": "NbtEUz9tTjXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"yolov8n.pt\"\n",
        "MODEL_RESOLUTION = 1280\n",
        "CONFIDENCE_THRESHOLD = 0.3\n",
        "IOU_THRESHOLD = 0.7\n",
        "\n",
        "SOURCE_VIDEO_PATH = \"/content/input.mp4\"\n",
        "TARGET_VIDEO_PATH = \"/content/output.mp4\"\n",
        "\n",
        "SPEED_LIMIT = 60  # km/h (used later)\n"
      ],
      "metadata": {
        "id": "6Z7p1oc5TlvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸŽ¯ Load YOLO Model and Video Information\n",
        "\n",
        "- Loads YOLOv8 model\n",
        "- Extracts video metadata\n",
        "- Creates frame generator\n",
        "- Initializes ByteTrack tracker\n"
      ],
      "metadata": {
        "id": "iiEy-aZlToq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO(MODEL_NAME)\n",
        "\n",
        "video_info = sv.VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n",
        "\n",
        "frame_generator = sv.get_video_frames_generator(\n",
        "    source_path=SOURCE_VIDEO_PATH\n",
        ")\n",
        "\n",
        "byte_track = sv.ByteTrack(frame_rate=video_info.fps)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLi3g1hRTqVA",
        "outputId": "4aa9924d-647b-4be0-a41c-e6cbc3ff0568"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolov8n.pt to 'yolov8n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 6.2MB 258.8MB/s 0.0s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“ Perspective Transformation\n",
        "\n",
        "This class converts image coordinates into a top-down view,\n",
        "allowing more accurate distance and speed calculation.\n"
      ],
      "metadata": {
        "id": "e7gtoeB-TryH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViewTransformer:\n",
        "    def __init__(self, source: np.ndarray, target: np.ndarray):\n",
        "        source = source.astype(np.float32)\n",
        "        target = target.astype(np.float32)\n",
        "        self.m = cv2.getPerspectiveTransform(source, target)\n",
        "\n",
        "    def transform_points(self, points: np.ndarray) -> np.ndarray:\n",
        "        if points.size == 0:\n",
        "            return points\n",
        "        reshaped = points.reshape(-1, 1, 2).astype(np.float32)\n",
        "        transformed = cv2.perspectiveTransform(reshaped, self.m)\n",
        "        return transformed.reshape(-1, 2)\n"
      ],
      "metadata": {
        "id": "iljnyZutTtT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“ Define Source and Target Regions\n",
        "\n",
        "These points define:\n",
        "- Source: road plane in camera view\n",
        "- Target: bird's-eye mapped plane\n"
      ],
      "metadata": {
        "id": "AhqKvsbeTv3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SOURCE = np.array([\n",
        "    [400, 720],\n",
        "    [880, 720],\n",
        "    [1200, 200],\n",
        "    [80, 200]\n",
        "])\n",
        "\n",
        "TARGET = np.array([\n",
        "    [0, 720],\n",
        "    [400, 720],\n",
        "    [400, 0],\n",
        "    [0, 0]\n",
        "])\n",
        "\n",
        "view_transformer = ViewTransformer(\n",
        "    source=SOURCE,\n",
        "    target=TARGET\n",
        ")\n"
      ],
      "metadata": {
        "id": "HY1zxS3bTuin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ–Šï¸ Annotation Configuration\n",
        "\n",
        "Initializes bounding boxes, labels, and trace drawing\n",
        "with resolution-adaptive scaling.\n"
      ],
      "metadata": {
        "id": "qZRlvXq3Tyyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "width, height = video_info.resolution_wh\n",
        "\n",
        "thickness = max(1, int((width + height) / 1000))\n",
        "text_scale = max(0.4, (width + height) / 3000)\n",
        "\n",
        "box_annotator = sv.BoxAnnotator(thickness=thickness)\n",
        "\n",
        "label_annotator = sv.LabelAnnotator(\n",
        "    text_scale=text_scale,\n",
        "    text_thickness=thickness,\n",
        "    text_position=sv.Position.BOTTOM_CENTER\n",
        ")\n",
        "\n",
        "trace_annotator = sv.TraceAnnotator(\n",
        "    thickness=thickness,\n",
        "    trace_length=int(video_info.fps * 2),\n",
        "    position=sv.Position.BOTTOM_CENTER\n",
        ")\n"
      ],
      "metadata": {
        "id": "ug5-yxrgT0Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ§® Speed Estimation Setup\n",
        "\n",
        "Tracks Y-coordinate displacement across frames\n",
        "and converts pixel movement into km/h.\n"
      ],
      "metadata": {
        "id": "8EiM2aaIT5A7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coordinates = defaultdict(\n",
        "    lambda: deque(maxlen=int(video_info.fps))\n",
        ")\n",
        "\n",
        "PIXELS_PER_METER = 20  # tune once\n"
      ],
      "metadata": {
        "id": "TKe20FcrT6By"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸš€ Process Video Frames\n",
        "\n",
        "For each frame:\n",
        "- Detect vehicles\n",
        "- Track objects\n",
        "- Transform coordinates\n",
        "- Estimate speed\n",
        "- Annotate frame\n",
        "- Save output\n"
      ],
      "metadata": {
        "id": "GroT5Qx7T7u5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:\n",
        "    for frame in tqdm(frame_generator, total=video_info.total_frames):\n",
        "\n",
        "        result = model(frame, imgsz=MODEL_RESOLUTION, verbose=False)[0]\n",
        "        detections = sv.Detections.from_ultralytics(result)\n",
        "\n",
        "        detections = detections[detections.confidence > CONFIDENCE_THRESHOLD]\n",
        "        detections = detections[detections.class_id != 0]\n",
        "        detections = detections.with_nms(IOU_THRESHOLD)\n",
        "        detections = byte_track.update_with_detections(detections)\n",
        "\n",
        "        points = detections.get_anchors_coordinates(\n",
        "            anchor=sv.Position.BOTTOM_CENTER\n",
        "        )\n",
        "\n",
        "        points = view_transformer.transform_points(points).astype(int)\n",
        "\n",
        "        labels = [\"\"] * len(detections)\n",
        "\n",
        "        WINDOW = int(video_info.fps * 0.5)\n",
        "        MAX_PIXEL_JUMP = 25\n",
        "\n",
        "        for i, (tracker_id, point) in enumerate(\n",
        "            zip(detections.tracker_id, points)\n",
        "        ):\n",
        "            y = point[1]\n",
        "\n",
        "            if coordinates[tracker_id]:\n",
        "                last_y = coordinates[tracker_id][-1]\n",
        "                y = last_y + np.clip(\n",
        "                    y - last_y,\n",
        "                    -MAX_PIXEL_JUMP,\n",
        "                    MAX_PIXEL_JUMP\n",
        "                )\n",
        "\n",
        "            coordinates[tracker_id].append(y)\n",
        "\n",
        "            if len(coordinates[tracker_id]) < WINDOW:\n",
        "                continue\n",
        "\n",
        "            y_values = list(coordinates[tracker_id])[-WINDOW:]\n",
        "            pixel_distance = abs(y_values[-1] - y_values[0])\n",
        "\n",
        "            time_seconds = WINDOW / video_info.fps\n",
        "            meters = pixel_distance / PIXELS_PER_METER\n",
        "            speed_kmh = (meters / time_seconds) * 3.6\n",
        "\n",
        "            if 3 <= speed_kmh <= 140:\n",
        "                labels[i] = f\"{int(speed_kmh)} km/h\"\n",
        "\n",
        "        frame = trace_annotator.annotate(frame, detections)\n",
        "        frame = box_annotator.annotate(frame, detections)\n",
        "        frame = label_annotator.annotate(frame, detections, labels)\n",
        "\n",
        "        sink.write_frame(frame)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWB_wR5fT9QU",
        "outputId": "aa44b1c6-f2ee-4056-efa2-7813d37d8a45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 722/722 [07:36<00:00,  1.58it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âœ… Processing Complete\n",
        "\n",
        "The output video has been successfully generated\n",
        "with speed annotations.\n"
      ],
      "metadata": {
        "id": "hzxTV3ANT-2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"âœ… DONE. Output saved at:\", TARGET_VIDEO_PATH)\n"
      ],
      "metadata": {
        "id": "wsJzvHlLUAT7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}